{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2ca08cd-3e3c-4b79-82de-e0f3aaa722ab",
   "metadata": {},
   "source": [
    "# Experiments with warm start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d58d464-d210-4f6e-acf8-6955f430aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments_utils import run_maybe_remotely\n",
    "from sbi_ebm.sbibm.sbi_ebm import run as run_unle\n",
    "for seed in (1, 2, 3, 4, 5)[:1]:\n",
    "    for task in (\"two_moons\", \"slcp\", \"lotka_volterra\", \"gaussian_linear_uniform\"):\n",
    "        for ns in (\n",
    "            (100,) * 10,\n",
    "            (1000,) * 10,\n",
    "            (10000,) * 10,\n",
    "        ):\n",
    "            for no in list(range(1, 10)):\n",
    "                _ = run_maybe_remotely(\n",
    "                    run_unle,\n",
    "                    folder_name=\"icml\",\n",
    "                    experience_name=\"sbibm_sunle\",\n",
    "                    use_slurm=True,\n",
    "                    slurm_kwargs={\n",
    "                        \"exclude\": \"gpu-350-01,gpu-380-11\",\n",
    "                    },\n",
    "                    # \"two_moons\", (1000,), 1,\n",
    "                    task=task,\n",
    "                    num_samples=ns,\n",
    "                    num_observation=no,\n",
    "                    num_smc_steps=5,\n",
    "                    num_mala_steps=50,\n",
    "                    # num_mala_steps=200,\n",
    "                    use_warm_start=True,\n",
    "                    learning_rate=0.001 if task == \"lotka_volterra\" else 0.01,\n",
    "                    max_iter=500,\n",
    "                    # max_iter=2000,\n",
    "                    weight_decay=0.1,\n",
    "                    random_seed=seed,\n",
    "                    sampler=\"mala\",\n",
    "                    num_particles=1000,\n",
    "                    batch_size=1000,\n",
    "                    restart_every=None,\n",
    "                    num_posterior_samples=10000,\n",
    "                    use_nuts=False,\n",
    "                    init_proposal=\"prior\",\n",
    "                    # init_proposal=\"prior\",\n",
    "                    noise_injection_val=0.0005,\n",
    "                    # proposal=\"prior+noise\",\n",
    "                    proposal=\"data\",\n",
    "                    inference_sampler=\"exchange_mcmc\",\n",
    "                    ebm_model_type=\"likelihood\",\n",
    "                    select_based_on_test_loss=False,\n",
    "                    inference_proposal=\"prior\",\n",
    "                    use_data_from_past_rounds=True,\n",
    "                    # inference_num_warmup_steps=2000,\n",
    "                    inference_num_warmup_steps=500,\n",
    "                    exchange_mcmc_inner_sampler_num_steps=100,\n",
    "                    evaluate_posterior=True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da0c2eb-09b8-4354-9f20-e902d4b2970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments_utils import run_maybe_remotely\n",
    "from sbi_ebm.sbibm.sbi_ebm import run as run_unle\n",
    "for seed in (1, 2, 3, 4, 5)[:1]:\n",
    "    for task in (\"slcp\", \"lotka_volterra\", \"gaussian_linear_uniform\", \"two_moons\"):\n",
    "        for ns in (\n",
    "            (100,) * 10,\n",
    "            (1000,) * 10,\n",
    "            (10000,) * 10,\n",
    "        ):\n",
    "            for no in list(range(1, 10)):\n",
    "                _ = run_maybe_remotely(\n",
    "                    run_unle,\n",
    "                    folder_name=\"icml\",\n",
    "                    experience_name=\"sbibm_sunle_divi\",\n",
    "                    use_slurm=True,\n",
    "                    slurm_kwargs={ },\n",
    "                    # \"two_moons\", (1000,), 1,\n",
    "                    task=task,\n",
    "                    num_samples=ns,\n",
    "                    num_observation=no,\n",
    "                    num_smc_steps=5,\n",
    "                    num_mala_steps=50,\n",
    "                    # num_mala_steps=200,\n",
    "                    use_warm_start=True,\n",
    "                    learning_rate=0.001 if task == \"lotka_volterra\" else 0.01,\n",
    "                    max_iter=500,\n",
    "                    # max_iter=2000,\n",
    "                    weight_decay=0.1,\n",
    "                    random_seed=seed,\n",
    "                    sampler=\"mala\",\n",
    "                    num_particles=1000,\n",
    "                    batch_size=1000,\n",
    "                    restart_every=None,\n",
    "                    num_posterior_samples=10000,\n",
    "                    use_nuts=False,\n",
    "                    init_proposal=\"prior\",\n",
    "                    # init_proposal=\"prior\",\n",
    "                    noise_injection_val=0.0005,\n",
    "                    # proposal=\"prior+noise\",\n",
    "                    proposal=\"data\",\n",
    "                    inference_sampler=\"mala\",\n",
    "                    ebm_model_type=\"likelihood\",\n",
    "                    select_based_on_test_loss=False,\n",
    "                    inference_proposal=\"prior\",\n",
    "                    use_data_from_past_rounds=True,\n",
    "                    # inference_num_warmup_steps=2000,\n",
    "                    inference_num_warmup_steps=500,\n",
    "                    exchange_mcmc_inner_sampler_num_steps=100,\n",
    "                    evaluate_posterior=True,\n",
    "                    estimate_log_normalizer=True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa2256-5bb4-4ff3-92d4-7890735fbfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments_utils import run_maybe_remotely\n",
    "from sbi_ebm.sbibm.sbi_ebm import run as run_unle\n",
    "for seed in (1, 2, 3, 4, 5)[1:]:\n",
    "    for task in (\"slcp\", \"lotka_volterra\", \"gaussian_linear_uniform\", \"two_moons\"):\n",
    "        for ns in (\n",
    "            (100,) * 10,\n",
    "            (1000,) * 10,\n",
    "            (10000,) * 10,\n",
    "        ):\n",
    "            for no in list(range(1, 10)):\n",
    "                _ = run_maybe_remotely(\n",
    "                    run_unle,\n",
    "                    folder_name=\"icml\",\n",
    "                    experience_name=\"paper\",\n",
    "                    use_slurm=True,\n",
    "                    slurm_kwargs={},\n",
    "                    # \"two_moons\", (1000,), 1,\n",
    "                    task=task,\n",
    "                    num_samples=ns,\n",
    "                    num_observation=no,\n",
    "                    num_smc_steps=5,\n",
    "                    num_mala_steps=3,\n",
    "                    # num_mala_steps=200,\n",
    "                    use_warm_start=True,\n",
    "                    learning_rate=0.001 if task == \"lotka_volterra\" else 0.01,\n",
    "                    max_iter=2000,\n",
    "                    weight_decay=0.1,\n",
    "                    random_seed=seed,\n",
    "                    sampler=\"smc\",\n",
    "                    num_particles=1000,\n",
    "                    batch_size=1000,\n",
    "                    restart_every=None,\n",
    "                    num_posterior_samples=10000,\n",
    "                    use_nuts=False,\n",
    "                    init_proposal=\"prior\",\n",
    "                    # init_proposal=\"prior\",\n",
    "                    noise_injection_val=0.0005,\n",
    "                    # proposal=\"prior+noise\",\n",
    "                    proposal=\"data\",\n",
    "                    inference_sampler=\"smc\",\n",
    "                    ebm_model_type=\"joint_tilted\",\n",
    "                    select_based_on_test_loss=False,\n",
    "                    inference_proposal=\"prior\",\n",
    "                    use_data_from_past_rounds=True,\n",
    "                    # inference_num_warmup_steps=2000,\n",
    "                    inference_num_warmup_steps=500,\n",
    "                    exchange_mcmc_inner_sampler_num_steps=100,\n",
    "                    evaluate_posterior=True\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unle-icml-2023",
   "language": "python",
   "name": "unle-icml-2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
